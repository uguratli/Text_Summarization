{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/homebrew/lib/python3.11/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/homebrew/lib/python3.11/site-packages (from fasttext) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from fasttext) (67.6.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from fasttext) (1.26.1)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: snowballstemmer in /opt/homebrew/lib/python3.11/site-packages (2.2.0)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install fasttext\n",
    "! pip3 install nltk\n",
    "! pip3 install snowballstemmer\n",
    "! pip3 install sklearn\n",
    "! pip3 install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from snowballstemmer import TurkishStemmer\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import news\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('tr', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.tr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_it(x):\n",
    "    \"\"\"Turkish stemmer.\n",
    "    Returns stemmed words in given string, uses Snowball Turkish stemmer.\"\"\"\n",
    "    return [turkStem.stemWord(w) for w in x.split()]\n",
    "\n",
    "def remove_digit(text):\n",
    "    \"\"\"Digit cleaner.\n",
    "    Removes digits in given string.\"\"\"\n",
    "    digit_pattern = str.maketrans('', '', string.digits)\n",
    "    return text.translate(digit_pattern)\n",
    "\n",
    "def lower_it(text):\n",
    "    \"\"\"Lowers characters in given string.\n",
    "    Lowers all in string.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctiations(text):\n",
    "    \"\"\"Punction cleaner.\n",
    "    Removes punctions in given string.\"\"\"\n",
    "    pattern = str.maketrans('', '', string.punctuation)\n",
    "    text= text.translate(pattern)\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "def remove_stopwords(text, language='turkish'):\n",
    "    \"\"\"Stopword cleaner.\n",
    "    Removes stopwords in given string, language is set to Turkish by default.\"\"\"\n",
    "    st = stopwords.words(language)\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(st) + r')\\b\\s*')\n",
    "    text = pattern.sub('',text)\n",
    "    return text\n",
    "\n",
    "def replace_turkish(text):\n",
    "    \"\"\"Turkish Character Fixer.\n",
    "    Replaces Turkish character with English ones.\"\"\"\n",
    "    character_map = {'ç': 'c',\n",
    "                     'Ç': 'C',\n",
    "                     'ğ': 'g',\n",
    "                     'Ğ': 'G',\n",
    "                     'ı': 'i',\n",
    "                     'I': 'I',\n",
    "                     'ö': 'o',\n",
    "                     'Ö': 'O',\n",
    "                     'ş': 's',\n",
    "                     'Ş': 'S', \n",
    "                     'ü': 'u',\n",
    "                     'Ü': 'U'}\n",
    "    for initial, final in character_map.items():\n",
    "        text = text.replace(initial, final)\n",
    "    return text\n",
    "\n",
    "def sentence_it(x):\n",
    "    \"\"\"Sentence maker.\n",
    "    Returns one string form of given words array.\"\"\"\n",
    "    sentence = ''\n",
    "    for w in x.split(): sentence += w + \" \"\n",
    "    return sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preproces(text, stemmer=False):\n",
    "    \"\"\"String cleaner.\n",
    "    Removes digits, lowers charactes, removes punctiations and stop words in given string.\"\"\"\n",
    "    text = remove_digit(text)\n",
    "    text = lower_it(text)\n",
    "    text = remove_punctiations(text)\n",
    "    text = remove_stopwords(text)\n",
    "    #text = replace_turkish(text) # Since FastText includes Turkish characters, there is no need to replace them.\n",
    "    # But for different libraries, one may have to change them.\n",
    "    if stemmer:\n",
    "        text = stem_it(text)\n",
    "        text = sentence_it(text)\n",
    "    text = sentence_it(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    \"\"\"Text cleaner.\n",
    "    Returns sentences and cleaned sentences.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    clean_sentenes = [preproces(sentence) for sentence in sentences]\n",
    "    return sentences, clean_sentenes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence):\n",
    "    return sum([ft.get_word_vector(word) for word in sentence])/(len(sentence)+0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vectors(text):\n",
    "    \"\"\"Word vector creator.\n",
    "    Uses cleaned text array and returns word vector array\"\"\"\n",
    "    sentence_words = [[w for w in s.split()] for s in text]\n",
    "    word_vectors = [sentence_vector(s_words) for s_words in sentence_words]\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(text_vector):\n",
    "    \"\"\"Similarty Matrix generator.\n",
    "    Creates similarity matrix using sentence vectors via cosine similarity\"\"\"\n",
    "    sim_mat = np.zeros([len(text_vector), len(text_vector)])\n",
    "    for i in range(len(text_vector)):\n",
    "        for j in range(len(text_vector)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(text_vector[i].reshape(1, -1), text_vector[j].reshape(1, -1))[0, 0]\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(sentences, similarity_matrix):\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    text_dict = {}\n",
    "    for i in range(len(sentences)):\n",
    "        text_dict.update({str(i):{\"score\": scores[i],\n",
    "                                  \"text\": sentences[i]}})\n",
    "    text_order = sorted(text_dict, key=lambda x: text_dict[x][\"score\"], reverse=True)\n",
    "    extract = \"\"\n",
    "    for i in range(int(len(sentences)/3)):\n",
    "        id = text_order[i]\n",
    "        extract = extract + \" \" + text_dict[id][\"text\"]\n",
    "    return extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_extraction(text):\n",
    "    sentences, cleaned_sentences = text_cleaning(text)\n",
    "    sentence_vectors = word_vectors(cleaned_sentences)\n",
    "    sim_mat = similarity_matrix(sentence_vectors)\n",
    "    return extraction(sentences, sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İsrail İran’ın‘misilleme’saldırısına karşılık vermekten‘son anda’vazgeçti. İran, Suriye’deki büyükelçilik binasını vuran İsrail’e karşı‘misilleme’saldırısı başlatmış, İsrail’e yüzlerce insansız hava aracı  ve füze atılmıştı. İran‘Şimdilik bu kadar’mesajı verince gerilim korkulanın aksinedüşmüştü. İsrail’in İran’ın misillemesinden neredeyse hiç zarar görmemesi gerilimi daha da azalttı.  Yine de İsrail’in olası bir karşı saldırısının gerilimi yeniden tırmandırmasından kaygı duyuluyordu. ABD Başkanı Joe Biden’ın saldırıdan sonra telefonla görüştüğü İsrail Başbakanı Binyamin Netanyahu’dan herhangi bir karşılık verilmemesini istediği öne sürülmüştü. İsrail devlet televizyonu KAN, Netanyahu yönetiminin İran’ın saldırılarına karşılık vermekten‘son anda’bildirdi. Savaş kabinesi ve bakanlar kurulunun önce salt çoğunlukla İran’a derhal karşılık verilmesini onayladığı ancak Biden’la görüşme sonrası bu kararın değiştiği kaydedildi. \n"
     ]
    }
   ],
   "source": [
    "article = news.get_article()\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yine de İsrail’in olası bir karşı saldırısının gerilimi yeniden tırmandırmasından kaygı duyuluyordu. Savaş kabinesi ve bakanlar kurulunun önce salt çoğunlukla İran’a derhal karşılık verilmesini onayladığı ancak Biden’la görüşme sonrası bu kararın değiştiği kaydedildi.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_extraction(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
